{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %matplotlib qt\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from networkAlignmentAnalysis.models.registry import get_model\n",
    "from networkAlignmentAnalysis.datasets import get_dataset\n",
    "from networkAlignmentAnalysis.experiments.registry import get_experiment\n",
    "from networkAlignmentAnalysis import utils\n",
    "from networkAlignmentAnalysis import files\n",
    "from networkAlignmentAnalysis import train\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('using device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1.1. include additional AlignmentModel methods stored in extra class in base model\n",
    "# 5. SLURM!!!!\n",
    "\n",
    "# Basic alignment_comparison Analyses (or maybe for alignment_stats):\n",
    "# - compare initial to final alignment...\n",
    "# - compare initial alignment to delta weight norm...\n",
    "# - observe alignment of delta weight\n",
    "# - compare alignment to outgoing delta weight norm!\n",
    "\n",
    "# Eigenfeature analyses:\n",
    "# done: - start by just looking at amplitude of activity on each eigenvector within each layer\n",
    "# done: - Determine contribution of each eigenfeature on performance with a eigenvector dropout experiment\n",
    "# - Measure beta_adversarial (figure out how adversarial examples map onto eigenvectors)\n",
    "\n",
    "# forward_eigenvector_dropout is slow... maybe because cpu->gpu overhead? \n",
    "\n",
    "# alignmentShaping.ipynb has an adversarial experiment worth looking at\n",
    "# need to integrate manual shaping!!!!\n",
    "\n",
    "# Consider Valentin's idea about measuring an error threshold given signal and noise for a given level of alignment\n",
    "# e.g. plot a 2d heatmap comparing the noise amplitude and the average alignment\n",
    "# and then think about how to apply this to network design...\n",
    "\n",
    "# convert batch_cov to allow for batch_corr too (and make it \"smart\" with zero-var handling)\n",
    "# integrate batched alignment into pipeline  (there's test code in the tests directory)\n",
    "\n",
    "# Experiment Idea:\n",
    "# train a few networks on a task- show that you can predict the class from the loadings onto eigenvectors of hidden layers.\n",
    "# - see if this works for networks trained with a different regularizer (e.g. compare dropout, weight-decay, and none!)\n",
    "# J: equalize class numbers\n",
    "# -- can also use quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "file = files.dataset_path(\"ImageNet\")\n",
    "\n",
    "dist_params = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "center_crop = 224\n",
    "\n",
    "use_transforms = [\n",
    "    # Convert PIL Image to PyTorch Tensor\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    # Normalize inputs to canonical distribution\n",
    "    transforms.CenterCrop(center_crop),\n",
    "    transforms.Normalize((dist_params['mean'],), (dist_params['std'],)),\n",
    "    transforms.Resize(256),\n",
    "]\n",
    "transform = transforms.Compose(use_transforms)    \n",
    "\n",
    "kwargs = dict(\n",
    "    split='val',\n",
    "    root=file,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "imagenet = torchvision.datasets.ImageNet(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (224) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimagenet\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torchvision/datasets/folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torchvision/transforms/v2/_container.py:51\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m needs_unpacking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(inpt, params) \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torchvision/transforms/v2/_misc.py:165\u001b[0m, in \u001b[0;36mNormalize._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: Any, params: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:35\u001b[0m, in \u001b[0;36mTransform._call_kernel\u001b[0;34m(self, functional, inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, functional: Callable, inpt: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     34\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m _get_kernel(functional, \u001b[38;5;28mtype\u001b[39m(inpt), allow_passthrough\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_utils.py:31\u001b[0m, in \u001b[0;36m_kernel_tv_tensor_wrapper.<locals>.wrapper\u001b[0;34m(inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(kernel)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(inpt, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# If you're wondering whether we could / should get rid of this wrapper,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# lost after the first operation due to our own __torch_function__\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# logic.\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tv_tensors\u001b[38;5;241m.\u001b[39mwrap(output, like\u001b[38;5;241m=\u001b[39minpt)\n",
      "File \u001b[0;32m~/.conda/envs/networkAlignmentAnalysis/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_misc.py:63\u001b[0m, in \u001b[0;36mnormalize_image\u001b[0;34m(image, mean, std, inplace)\u001b[0m\n\u001b[1;32m     61\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msub_(mean)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (224) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "print(imagenet[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AlexNet'\n",
    "dataset_name = 'ImageNet'\n",
    "\n",
    "net = get_model(model_name, build=True, dataset=dataset_name, dropout=0.5)\n",
    "\n",
    "loader_parameters = dict(\n",
    "    shuffle=True,\n",
    ")\n",
    "dataset = get_dataset(dataset_name, build=True, transform_parameters=net, loader_parameters=loader_parameters, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageNet\n",
       "    Number of datapoints: 1281167\n",
       "    Root location: /n/holyscratch01/bsabatini_lab/Lab/ImageNet\n",
       "    Split: train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "                 ToImage()\n",
       "                 ToDtype(scale=True)\n",
       "                 Normalize(mean=[[0.485, 0.456, 0.406]], std=[[0.229, 0.224, 0.225]], inplace=False)\n",
       "                 Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 100%|██████████| 10/10 [02:35<00:00, 15.51s/it]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(95.9620)]\n"
     ]
    }
   ],
   "source": [
    "results = train.train(nets, optimizers, dataset, num_epochs=50, alignment=False)\n",
    "test_results = train.test(nets, dataset, train=False, alignment=False)\n",
    "\n",
    "all_images, _ = utils.named_transpose([net._process_collect_activity(dataset, train_set=False, with_updates=False) for net in nets])\n",
    "_, _, eigenvectors = utils.named_transpose([net.measure_eigenfeatures(images, with_updates=False)\n",
    "                                            for net, images in zip(nets, all_images)])\n",
    "\n",
    "# dropout_results = train.eigenvector_dropout([net], dataset, [eigenvalue], [eigenvector], train_set=False, by_layer=True)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(results['accuracy'])\n",
    "plt.show()\n",
    "\n",
    "print(test_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "fgsm_transform = lambda x: x\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad, transform, sign):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    if sign:\n",
    "        data_grad = data_grad.sign()\n",
    "    else:\n",
    "        data_grad = data_grad.clone()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = transform(perturbed_image)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n",
    "\n",
    "def get_beta(inputs, eigenvectors):\n",
    "    betas = [input.cpu() @ evec for input, evec in zip(inputs, eigenvectors)]\n",
    "    # betas = [torch.sqrt(torch.mean(beta**2, dim=0)) for beta in betas]\n",
    "    return betas\n",
    "\n",
    "@utils.test_nets\n",
    "def test(nets, dataset, epsilon, eigenvectors, sign=True, train=True):\n",
    "    num_eps = len(epsilon)\n",
    "    num_nets = len(nets)\n",
    "    accuracy = torch.zeros((num_nets, num_eps))\n",
    "    examples = [[[] for _ in range(num_eps)] for _ in range(num_nets)]\n",
    "    betas = [[torch.zeros((num_nets, evec.size(0))) for evec in eigenvectors[0]]\n",
    "             for _ in range(num_eps)]\n",
    "\n",
    "    # dataloader\n",
    "    dataloader = dataset.train_loader if train else dataset.test_loader\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input, labels = dataset.unwrap_batch(batch)\n",
    "        \n",
    "        inputs = [input.clone() for _ in range(num_nets)]\n",
    "\n",
    "        for input in inputs:\n",
    "            input.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        outputs = [net(input, store_hidden=True) for net, input in zip(nets, inputs)]\n",
    "        input_to_layers = [net.get_layer_inputs(input, precomputed=True) for net in nets]\n",
    "        init_preds = [torch.argmax(output,axis=1) for output in outputs] # find true prediction\n",
    "        least_likely = [torch.argmin(output,axis=1) for output in outputs] # find least likely digit according to model\n",
    "        \n",
    "        c_betas = utils.transpose_list([get_beta(input, evec) for input, evec in zip(input_to_layers, eigenvectors)])\n",
    "        s_betas = [torch.stack(cb) for cb in c_betas]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = [dataset.measure_loss(output, labels) for output in outputs]\n",
    "        # loss = dataset.measure_loss(output, least_likely)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        for net in nets:\n",
    "            net.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grads = [input.grad.data for input in inputs]\n",
    "        \n",
    "        for epsidx, eps in enumerate(epsilon):\n",
    "            \n",
    "            # Call FGSM Attack\n",
    "            perturbed_inputs = [fgsm_attack(input, eps, data_grad, fgsm_transform, sign)\n",
    "                                for input, data_grad in zip(inputs, data_grads)]\n",
    "\n",
    "            # Re-classify the perturbed image\n",
    "            outputs = [net(perturbed_input, store_hidden=True)\n",
    "                       for net, perturbed_input in zip(nets, perturbed_inputs)]\n",
    "            input_to_layers = [net.get_layer_inputs(perturbed_input, precomputed=True)\n",
    "                               for net, perturbed_input in zip(nets, perturbed_inputs)]\n",
    "            c_eps_betas = utils.transpose_list([get_beta(input, evec) for input, evec in zip(input_to_layers, eigenvectors)])\n",
    "            s_eps_betas = [torch.stack(ceb) for ceb in c_eps_betas]\n",
    "            d_eps_betas = [sebeta - sbeta for sebeta, sbeta in zip(s_eps_betas, s_betas)]\n",
    "            rms_betas = [torch.sqrt(torch.mean(db**2, dim=1)) for db in d_eps_betas]\n",
    "\n",
    "            for ii, rbeta in enumerate(rms_betas):\n",
    "                betas[epsidx][ii] += rbeta\n",
    "\n",
    "            # Check for success\n",
    "            final_preds = [torch.argmax(output, axis=1) for output in outputs]\n",
    "            accuracy[:, epsidx] += torch.tensor([sum(final_pred==labels).cpu() for final_pred in final_preds])\n",
    "            \n",
    "            # Idx where adversarial example worked\n",
    "            idx_success = [torch.where((init_pred==labels) & (final_pred != labels))[0].cpu()\n",
    "                           for init_pred, final_pred in zip(init_preds, final_preds)]\n",
    "            \n",
    "            adv_exs = [perturbed_input.detach().cpu().numpy() for perturbed_input in perturbed_inputs]\n",
    "            for ii, (adv_ex, idx, init_pred, final_pred) in enumerate(zip(adv_exs, idx_success, init_preds, final_preds)):\n",
    "                examples[ii][epsidx].append((init_pred[idx], final_pred[idx], adv_ex[idx]))\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    accuracy = accuracy / float(len(dataloader.dataset))\n",
    "\n",
    "    # Average across betas\n",
    "    betas = [[cb / float(len(dataloader.dataset)) for cb in beta] for beta in betas]\n",
    "        \n",
    "    # Return the accuracy and an adversarial example\n",
    "    return accuracy, betas, examples\n",
    "\n",
    "\n",
    "epsilons = np.linspace(0,1,31)\n",
    "# epsilons = np.hstack((0, np.logspace(2,4,5)))\n",
    "# epsilons = np.hstack((0, np.linspace(10,100000,21)))\n",
    "sign = True\n",
    "\n",
    "num_eps = len(epsilons)\n",
    "num_nets = len(nets)\n",
    "\n",
    "prtAccuracy = torch.zeros((num_eps, num_nets))\n",
    "newAccuracy = torch.zeros((num_eps, num_nets))\n",
    "\n",
    "# Run test for each epsilon\n",
    "acc, betas, ex = test(nets, dataset, epsilons, eigenvectors, sign=sign, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['full', 'dropout']\n",
    "\n",
    "plt.close('all')\n",
    "for name, a in zip(names, acc):\n",
    "    plt.plot(epsilons, a, label=name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, len(betas[0]), figsize=(12, 3), layout='constrained')\n",
    "for ii, beta in enumerate(betas[10]):\n",
    "    ax[ii].plot(range(beta.size(1)), savgol_filter(beta[0].detach() / torch.sum(beta[0].detach()), 5, 1), label='vanilla')\n",
    "    ax[ii].plot(range(beta.size(1)), savgol_filter(beta[1].detach() / torch.sum(beta[1].detach()), 5, 1), label='dropout')\n",
    "    ax[ii].set_xlabel('PC Dimension')\n",
    "    ax[ii].set_ylabel('Projection of adversarial perturbation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(betas[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkAlignmentAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
